# Tensors
-> Lots of interop functions between numpy and torch
* NB: a view is created, not a copy, so action on one modifies the
* other (i.e. same memory location)

# Autograd
-> Autograd automates the differentian (for back prop)
* Set Tensor property .requires_grad to True
* Call .backward() on the tensor to compute its derivative
* You can then access the grad with Tensor.grad
* NB: but for e.g. eval runs you want this off - so wrap code with "with
torch.no_grad():"

# Networks
-> all nets inherit from nn.Module
-> call print(net) to get summary of its layers (defined in
constructor)
-> NB: torch.nn only supports mini-batches.
* The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.
* For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.
* If you have a single sample, just use input.unsqueeze(0) to add a
fake batch dimension.

# Loss
-> nn contains loss functions
-> instantiate and then compute by passing y_pred and y
-> call loss.grad_fn to see its loss graph?
-> print(loss.grad_fn)  # MSELoss
   print(loss.grad_fn.next_functions[0][0])  # Linear
   print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  #
   ReLU

# Backprop
-> always zero gradient buffers? net.zero_grad()
-> then simply call loss.backward()
-> Weight update by calling either
--> learning_rate = 0.01
    for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
--> OR,
    use an optimizer - see - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#update-the-weights

# Exampe of GPU support
- https://github.com/pytorch/examples/blob/master/mnist/main.py


